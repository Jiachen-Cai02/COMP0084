{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OGGNkLXL2Bqk",
        "outputId": "78f7d0c8-51f4-4391-d3f0-f3b93db75f77"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "import re\n",
        "import string\n",
        "from operator import itemgetter\n",
        "import csv\n",
        "from collections import Counter\n",
        "import time\n",
        "import random\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models.word2vec import LineSentence\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import xgboost as xgb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "f12xdljA2Bqm"
      },
      "outputs": [],
      "source": [
        "def loadtext(filepath):\n",
        "    with open(filepath, 'r') as f:\n",
        "        txt = []\n",
        "        for line in f.readlines():\n",
        "            txt.append(line.strip().lower())\n",
        "        \n",
        "    return txt\n",
        "\n",
        "def preprocessing(text, remove):\n",
        "    #initial setting\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    stemmer = SnowballStemmer('english')\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokenizer = RegexpTokenizer(r'\\s+', gaps = True)\n",
        "    processed_txt = []\n",
        "\n",
        "    for line in text:\n",
        "        sentence = []\n",
        "        #remove the punctuation\n",
        "        table = str.maketrans(dict.fromkeys(string.punctuation))\n",
        "        line = line.translate(table)\n",
        "\n",
        "        line = re.sub(r'[^a-zA-Z\\s]', u' ', line, flags = re.UNICODE)\n",
        "\n",
        "        line_token = tokenizer.tokenize(line)\n",
        "\n",
        "        #remove stop words\n",
        "        if (remove == True):\n",
        "            line_token = [w for w in line_token if not w in stop_words]\n",
        "        \n",
        "\n",
        "        for word in line_token:\n",
        "            word = lemmatizer.lemmatize(word)\n",
        "            word = stemmer.stem(word)\n",
        "            sentence.append(word)\n",
        "\n",
        "        processed_txt.append(sentence)\n",
        "    \n",
        "    return processed_txt\n",
        "\n",
        "def inverted_index(pid_data, processed_data):\n",
        "\n",
        "    dic = {}\n",
        "    \n",
        "    for i in range(len(pid_data)):\n",
        "        processed_passage = processed_data[i]\n",
        "\n",
        "        for token in processed_passage:\n",
        "            token_num = processed_passage.count(token)\n",
        "\n",
        "            if token not in dic.keys():\n",
        "                dic[token] = {pid_data[i]: token_num}\n",
        "            else:\n",
        "                add_num = {pid_data[i]: token_num}\n",
        "                dic[token].update(add_num)\n",
        "\n",
        "    return dic\n",
        "\n",
        "\n",
        "def generate_rel_dict(qid_list, pid_list, rel_list):\n",
        "    '''\n",
        "    generate two dict according to the given data\n",
        "    '''\n",
        "\n",
        "    rel_dict = {}\n",
        "    non_rel_dict = {}\n",
        "\n",
        "    for i in range(len(qid_list)):\n",
        "        qid = qid_list[i]\n",
        "        pid = pid_list[i]\n",
        "        rel = rel_list[i]\n",
        "\n",
        "        if rel > 0:\n",
        "            add_dict = {pid:i}\n",
        "            if qid in rel_dict.keys():\n",
        "                rel_dict[qid].update(add_dict)\n",
        "            else:\n",
        "                rel_dict[qid] = add_dict\n",
        "        else:\n",
        "            add_dict = {pid:i}\n",
        "            if qid in non_rel_dict.keys():\n",
        "                non_rel_dict[qid].update(add_dict)\n",
        "            else:\n",
        "                non_rel_dict[qid] = add_dict\n",
        "\n",
        "    return rel_dict, non_rel_dict\n",
        "\n",
        "def generate_AP(model, rel_dic, non_rel_dic):\n",
        "    \n",
        "    total_ap = []\n",
        "    qid_list = list(model.keys())\n",
        "\n",
        "    for qid in qid_list:\n",
        "        N = 0\n",
        "        R_rel = 0\n",
        "        precision = 0\n",
        "        model_pid_list= list(model[qid].keys())\n",
        "        rel_pid_list = list(rel_dic[qid].keys())\n",
        "        \n",
        "\n",
        "        if len(set(model_pid_list) & set(rel_pid_list)) == 0:\n",
        "            precision = 0\n",
        "            total_ap.append(precision)\n",
        "\n",
        "        else:\n",
        "            for pid in model_pid_list:\n",
        "                N += 1\n",
        "                if pid in rel_pid_list:\n",
        "                    R_rel += 1\n",
        "                    precision += R_rel/N\n",
        "                else:\n",
        "                    continue\n",
        "                if R_rel == len(rel_dic[qid]):\n",
        "                    break\n",
        "\n",
        "        \n",
        "            total_ap.append(precision / R_rel)\n",
        "    \n",
        "    return total_ap\n",
        "\n",
        "def generate_NDCG(model, rel_dic, non_rel_dic):\n",
        "\n",
        "    NDCG = []\n",
        "    qid_list = list(model.keys())\n",
        "    \n",
        "    for qid in qid_list:\n",
        "        N = 0\n",
        "        N_opt = 0\n",
        "        DCG = 0\n",
        "        DCG_opt = 0\n",
        "        model_pid_list = list(model[qid].keys())\n",
        "        rel_pid_list = list(rel_dic[qid].keys())\n",
        "\n",
        "        if len(set(model_pid_list) & set(rel_pid_list)) == 0:\n",
        "            DCG += 0\n",
        "        \n",
        "\n",
        "        else:\n",
        "            rel_qid_dic = rel_dic[qid]\n",
        "            for pid in model_pid_list:\n",
        "                N += 1\n",
        "                if pid in rel_pid_list:\n",
        "                    rel_pid = 1\n",
        "                else:\n",
        "                    rel_pid = 0\n",
        "            \n",
        "                DCG += (2**rel_pid - 1)/np.log(1 + N)\n",
        "\n",
        "        #find the opt DCG\n",
        "        rel_qid_dic = rel_dic[qid]\n",
        "        best_sort_ranking = dict(sorted(rel_qid_dic.items(), key=itemgetter(1), reverse = True)[: 100])\n",
        "        opt_pid_list = list(best_sort_ranking.keys())\n",
        "        \n",
        "        for pid in opt_pid_list:\n",
        "            rel_pid = 1\n",
        "            N_opt += 1\n",
        "            DCG_opt += (2**rel_pid - 1)/np.log(1 + N_opt)\n",
        "\n",
        "        NDCG.append(DCG / DCG_opt)\n",
        "    \n",
        "\n",
        "    return NDCG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "uNhm8CJt2Bqn"
      },
      "outputs": [],
      "source": [
        "train_data = pd.read_csv('train_data.tsv', sep = '\\t', header = 0)\n",
        "validation_data = pd.read_csv('validation_data.tsv', sep = '\\t', header = 0)\n",
        "\n",
        "train_qid_dict = np.load('train_qid_dict.npy', allow_pickle= True).tolist()\n",
        "train_pid_dict = np.load('train_pid_dict.npy', allow_pickle= True).tolist()\n",
        "\n",
        "train_qid_list = list(train_data['qid'])\n",
        "train_pid_list = list(train_data['pid'])\n",
        "\n",
        "\n",
        "train_rel_list = list(train_data['relevancy'])\n",
        "train_rel_dic, train_non_rel_dic = generate_rel_dict(train_qid_list, train_pid_list, train_rel_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fxfxk41d2Bqn",
        "outputId": "9813d8bb-8679-4b9e-8436-6de89d13cfbc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "the number of revelance rows is 4797\n",
            "the number of non-revelance rows is 4359542\n"
          ]
        }
      ],
      "source": [
        "def get_row(dict):\n",
        "    key_list = list(dict.keys())\n",
        "    len_row = 0\n",
        "    for key in key_list:\n",
        "        len_row += len(dict[key])\n",
        "\n",
        "    return len_row\n",
        "\n",
        "print('the number of revelance rows is', get_row(train_rel_dic))#4797\n",
        "print('the number of non-revelance rows is', get_row(train_non_rel_dic))#4359542"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Owlkzu-h2Bqo",
        "outputId": "973a40bd-f215-444e-cbf2-c062314531d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "current length of new dataset is 109468\n",
            "            qid      pid                                            queries  \\\n",
            "0       1011811  4354002                     which is positive and negative   \n",
            "1        467597  6683905                           oak ridge is what county   \n",
            "2       1013797  8040628    what types of animals live in the tropical zone   \n",
            "3        118365  7153226                                    define biobanks   \n",
            "4        826518  6084159  what is the job description for insurance company   \n",
            "...         ...      ...                                                ...   \n",
            "109463  1085535  8102331                             cardiovascular meaning   \n",
            "109464  1087904   608735                           what age do moles appear   \n",
            "109465   472359  7571063                         paulding oh in what county   \n",
            "109466  1084076  2933694                         what does cilostazol treat   \n",
            "109467   960003  5930777                             when was stew invented   \n",
            "\n",
            "                                                  passage  relevancy  \n",
            "0       In first quadrant, both x and y co-ordinate ar...        0.0  \n",
            "1       Homefacts City Report. Ridge Manor is located ...        0.0  \n",
            "2       About the Bat Zone. Once located at the Cranbr...        0.0  \n",
            "3       Define attribute: a quality, character, or cha...        0.0  \n",
            "4       Recruiters employed by a staffing company may ...        0.0  \n",
            "...                                                   ...        ...  \n",
            "109463  Ascending aorta graft, with cardiopulmonary by...        0.0  \n",
            "109464  Chase Away Moles for Up to 75 Days - with Cast...        0.0  \n",
            "109465  Paulding, Ohio. Paulding is a village in and t...        1.0  \n",
            "109466  Knowledge center. Lyrica is Pfizer's trade nam...        0.0  \n",
            "109467  Peter Cooper invented the Tom Thumb.(or locomo...        0.0  \n",
            "\n",
            "[109468 rows x 5 columns]\n"
          ]
        }
      ],
      "source": [
        "def subsampling(pid_dict, non_rel_dic):\n",
        "    \n",
        "    qid_list = list(non_rel_dic.keys())\n",
        "\n",
        "\n",
        "    save_index_list = []\n",
        "    for qid in qid_list:\n",
        "\n",
        "        non_rel_pid_dict = non_rel_dic[qid]\n",
        "\n",
        "        non_rel_index_list = list(non_rel_pid_dict.values())\n",
        "        save_len = int(len(non_rel_index_list) * 0.025)\n",
        "        #shuffle the pid then delete by specific ratio\n",
        "        random.shuffle(non_rel_index_list)\n",
        "        new_non_rel_index_list = non_rel_index_list[0:save_len]\n",
        "\n",
        "        #upadte the pid revelant\n",
        "        save_index_list.extend(new_non_rel_index_list)\n",
        "\n",
        "        #new_rel_dic[qid] = add_non_rel\n",
        "\n",
        "    #new_pid_dict = {key:val for key, val in pid_dict.items() if key in save_pid_list}\n",
        "\n",
        "    return save_index_list\n",
        "\n",
        "new_non_index = subsampling(train_pid_dict, train_non_rel_dic)\n",
        "\n",
        "\n",
        "\n",
        "rel_index = []\n",
        "for qid in list(train_rel_dic.keys()):\n",
        "    rel_index_list = list(train_rel_dic[qid].values())\n",
        "    rel_index.extend(rel_index_list)\n",
        "\n",
        "new_train_index = new_non_index + rel_index\n",
        "print('current length of new dataset is', len(new_train_index))\n",
        "\n",
        "new_train_data = []\n",
        "for index in new_train_index:\n",
        "    new_train_data.append(train_data[index:index + 1])\n",
        "\n",
        "random.shuffle(new_train_data)\n",
        "\n",
        "new_train_data = pd.concat(new_train_data, axis = 0, ignore_index=True)\n",
        "\n",
        "print(new_train_data)\n",
        "\n",
        "train_passage = preprocessing(new_train_data['passage'], True)\n",
        "train_queries = preprocessing(new_train_data['queries'], True)\n",
        "\n",
        "\n",
        "train_passage_dict = dict(zip(new_train_data['pid'], train_passage))\n",
        "train_queries_dict = dict(zip(new_train_data['qid'], train_queries))\n",
        "\n",
        "validation_passage_dict = np.load('validation_pid_dict.npy', allow_pickle= True).tolist()\n",
        "validation_queries_dict = np.load('validation_qid_dict.npy', allow_pickle= True).tolist()\n",
        "\n",
        "validation_passage = preprocessing(validation_data['passage'], True)\n",
        "validation_queries = preprocessing(validation_data['queries'], True)\n",
        "\n",
        "validation_qid_list = list(validation_data['qid'])\n",
        "validation_pid_list = list(validation_data['pid'])\n",
        "\n",
        "validation_rel_list = list(validation_data['relevancy'])\n",
        "validation_rel_dic, validation_non_rel_dic = generate_rel_dict(validation_qid_list, validation_pid_list, validation_rel_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "8IzM7nT32Bqp"
      },
      "outputs": [],
      "source": [
        "with open('validation_passage.txt', 'w') as f:\n",
        "    for i in range(len(validation_passage)):\n",
        "        f.write(' '.join(validation_passage[i]) + '\\n')\n",
        "\n",
        "sentences = LineSentence('validation_passage.txt')\n",
        "model_validation_passage = Word2Vec(sentences, sg = 1, vector_size=100, window = 5, min_count=1, negative = 5, hs = 0, workers = 4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "qLTTXKcX2Bqp"
      },
      "outputs": [],
      "source": [
        "with open('validation_queries.txt', 'w') as f:\n",
        "    for i in range(len(validation_queries)):\n",
        "        f.write(' '.join(validation_queries[i]) + '\\n')\n",
        "\n",
        "sentences = LineSentence('validation_queries.txt')\n",
        "model_validation_queries = Word2Vec(sentences, sg = 1, vector_size=100, window = 5, min_count=1, negative = 5, hs = 0, workers = 4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "rkPPtPyO2Bqp"
      },
      "outputs": [],
      "source": [
        "with open('train_passage.txt', 'w') as f:\n",
        "    for i in range(len(train_passage)):\n",
        "        f.write(' '.join(train_passage[i]) + '\\n')\n",
        "\n",
        "sentences = LineSentence('train_passage.txt')\n",
        "model_train_passage = Word2Vec(sentences, sg = 1, vector_size=100, window = 5, min_count=1, negative = 5, hs = 0, workers = 4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "PO3TMENw2Bqp"
      },
      "outputs": [],
      "source": [
        "with open('train_queries.txt', 'w') as f:\n",
        "    for i in range(len(train_queries)):\n",
        "        f.write(' '.join(train_queries[i]) + '\\n')\n",
        "\n",
        "sentences = LineSentence('train_queries.txt')\n",
        "model_train_queries = Word2Vec(sentences, sg = 1, vector_size=100, window = 5, min_count=1, negative = 5, hs = 0, workers = 4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "5di8gFIU2Bqq"
      },
      "outputs": [],
      "source": [
        "def average_embedding(data_dict, model):\n",
        "\n",
        "    return_dic= {}\n",
        "    for i in list(data_dict.keys()):\n",
        "        data = data_dict[i]\n",
        "\n",
        "        if len(data) != 0:\n",
        "            token_vector = model.wv[data]\n",
        "            avg_vector = np.mean(token_vector, axis = 0)\n",
        "\n",
        "        add_dict = {i:avg_vector}\n",
        "        return_dic.update(add_dict)\n",
        "\n",
        "    return return_dic\n",
        "\n",
        "def generate_data_for_models(data, embedding_query_dict, embedding_passgae_dict, rel_dict):\n",
        "\n",
        "    qid_list = []\n",
        "    pid_list = []\n",
        "    queries_list = []\n",
        "    passgaes_list = []\n",
        "    rel_list = []\n",
        "    for i in range(len(data)):\n",
        "        qid = data.qid[i]\n",
        "        pid = data.pid[i]\n",
        "\n",
        "        if qid in embedding_query_dict.keys() and pid in embedding_passgae_dict.keys():\n",
        "            qid_list.append(qid)\n",
        "            pid_list.append(pid)\n",
        "            queries_list.append(embedding_query_dict[qid].reshape(-1))\n",
        "            passgaes_list.append(embedding_passgae_dict[pid].reshape(-1))\n",
        "\n",
        "            if pid in rel_dict[qid].keys():\n",
        "                rel = 1\n",
        "            else:\n",
        "                rel = 0\n",
        "            rel_list.append(rel)\n",
        "    \n",
        "    query_data = np.array(queries_list)\n",
        "    passages_data = np.array(passgaes_list)\n",
        "\n",
        "    x_data = np.concatenate((query_data, passages_data), axis = 1)\n",
        "    y_data = np.array(rel_list)\n",
        "    \n",
        "    return x_data, y_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RBdjjdE82Bqq",
        "outputId": "a22816b7-05fe-4d5e-d6f6-c6bae76f3230"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The size of x for training dataset is (109468, 200)\n",
            "The size of y for training dataset is (109468,)\n",
            "The size of x for validation dataset is (1103039, 200)\n",
            "The size of y for validation dataset is (1103039,)\n"
          ]
        }
      ],
      "source": [
        "train_queries_embedding = average_embedding(train_queries_dict, model_train_queries)\n",
        "train_passages_embedding = average_embedding(train_passage_dict, model_train_passage)\n",
        "\n",
        "train_x_data, train_y_data = generate_data_for_models(new_train_data, train_queries_embedding, train_passages_embedding, train_rel_dic)\n",
        "\n",
        "print(\"The size of x for training dataset is\", train_x_data.shape)\n",
        "print(\"The size of y for training dataset is\", train_y_data.shape)\n",
        "\n",
        "\n",
        "validation_queries_embedding = average_embedding(validation_queries_dict, model_validation_queries)\n",
        "validation_passages_embedding = average_embedding(validation_passage_dict, model_validation_passage)\n",
        "\n",
        "validation_x_data, validation_y_data = generate_data_for_models(validation_data, validation_queries_embedding, validation_passages_embedding, validation_rel_dic)\n",
        "\n",
        "print(\"The size of x for validation dataset is\", validation_x_data.shape)\n",
        "print(\"The size of y for validation dataset is\", validation_y_data.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "zVtyg_CuobNz"
      },
      "outputs": [],
      "source": [
        "def make_group(qid_list, xTr, yTr):\n",
        "  qid_array = np.array(qid_list)\n",
        "  idx = np.argsort(qid_array)\n",
        "  qid_array = qid_array[idx]\n",
        "  xTr = xTr[idx]\n",
        "  yTr = yTr[idx].reshape(-1, 1)\n",
        "  unique, count = np.unique(qid_array, return_counts = True)\n",
        "  group = list(count)\n",
        "               \n",
        "  return xTr, yTr, group\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fJy31O7V2Bqq",
        "outputId": "d50d3700-cdca-446c-a80e-956807b13c65"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/4 [00:00<?, ?it/s]\n",
            "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            " 33%|███▎      | 1/3 [02:40<05:21, 160.68s/it]\u001b[A\u001b[A\n",
            "\n",
            " 67%|██████▋   | 2/3 [05:59<03:03, 183.03s/it]\u001b[A\u001b[A\n",
            "\n",
            "100%|██████████| 3/3 [09:54<00:00, 198.01s/it]\n",
            "\n",
            " 33%|███▎      | 1/3 [09:54<19:48, 594.04s/it]\u001b[A\n",
            "\n",
            "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            " 33%|███▎      | 1/3 [05:10<10:20, 310.37s/it]\u001b[A\u001b[A\n",
            "\n",
            " 67%|██████▋   | 2/3 [11:31<05:52, 352.04s/it]\u001b[A\u001b[A\n",
            "\n",
            "100%|██████████| 3/3 [18:54<00:00, 378.17s/it]\n",
            "\n",
            " 67%|██████▋   | 2/3 [28:48<15:11, 911.97s/it]\u001b[A\n",
            "\n",
            "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            " 33%|███▎      | 1/3 [07:36<15:13, 456.94s/it]\u001b[A\u001b[A\n",
            "\n",
            " 67%|██████▋   | 2/3 [16:58<08:38, 518.71s/it]\u001b[A\u001b[A\n",
            "\n",
            "100%|██████████| 3/3 [27:57<00:00, 559.05s/it]\n",
            "\n",
            "100%|██████████| 3/3 [56:45<00:00, 1135.24s/it]\n",
            " 25%|██▌       | 1/4 [56:45<2:50:17, 3405.74s/it]\n",
            "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            " 33%|███▎      | 1/3 [02:43<05:27, 163.67s/it]\u001b[A\u001b[A\n",
            "\n",
            " 67%|██████▋   | 2/3 [06:05<03:05, 185.84s/it]\u001b[A\u001b[A\n",
            "\n",
            "100%|██████████| 3/3 [10:04<00:00, 201.47s/it]\n",
            "\n",
            " 33%|███▎      | 1/3 [10:04<20:08, 604.41s/it]\u001b[A\n",
            "\n",
            "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            " 33%|███▎      | 1/3 [05:22<10:44, 322.29s/it]\u001b[A\u001b[A\n",
            "\n",
            " 67%|██████▋   | 2/3 [11:56<06:04, 364.65s/it]\u001b[A\u001b[A\n",
            "\n",
            "100%|██████████| 3/3 [19:43<00:00, 394.54s/it]\n",
            "\n",
            " 67%|██████▋   | 2/3 [29:48<15:45, 945.13s/it]\u001b[A\n",
            "\n",
            "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            " 33%|███▎      | 1/3 [07:54<15:49, 474.69s/it]\u001b[A\u001b[A\n",
            "\n",
            " 67%|██████▋   | 2/3 [18:20<09:23, 563.70s/it]\u001b[A\u001b[A\n",
            "\n",
            "100%|██████████| 3/3 [29:49<00:00, 596.58s/it]\n",
            "\n",
            "100%|██████████| 3/3 [59:37<00:00, 1192.60s/it]\n",
            " 50%|█████     | 2/4 [1:56:23<1:56:53, 3506.95s/it]\n",
            "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            " 33%|███▎      | 1/3 [02:48<05:37, 168.96s/it]\u001b[A\u001b[A\n",
            "\n",
            " 67%|██████▋   | 2/3 [06:14<03:10, 190.68s/it]\u001b[A\u001b[A\n",
            "\n",
            "100%|██████████| 3/3 [10:19<00:00, 206.58s/it]\n",
            "\n",
            " 33%|███▎      | 1/3 [10:19<20:39, 619.75s/it]\u001b[A\n",
            "\n",
            "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            " 33%|███▎      | 1/3 [05:28<10:56, 328.44s/it]\u001b[A\u001b[A\n",
            "\n",
            " 67%|██████▋   | 2/3 [12:14<06:14, 374.01s/it]\u001b[A\u001b[A\n",
            "\n",
            "100%|██████████| 3/3 [20:14<00:00, 404.70s/it]\n",
            "\n",
            " 67%|██████▋   | 2/3 [30:33<16:09, 969.37s/it]\u001b[A\n",
            "\n",
            "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            " 33%|███▎      | 1/3 [07:49<15:38, 469.24s/it]\u001b[A\u001b[A\n",
            "\n",
            " 67%|██████▋   | 2/3 [17:32<08:56, 536.37s/it]\u001b[A\u001b[A\n",
            "\n",
            "100%|██████████| 3/3 [29:15<00:00, 585.10s/it]\n",
            "\n",
            "100%|██████████| 3/3 [59:49<00:00, 1196.39s/it]\n",
            " 75%|███████▌  | 3/4 [2:56:12<59:04, 3544.50s/it]  \n",
            "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            " 33%|███▎      | 1/3 [02:41<05:22, 161.21s/it]\u001b[A\u001b[A\n",
            "\n",
            " 67%|██████▋   | 2/3 [05:57<03:01, 181.88s/it]\u001b[A\u001b[A\n",
            "\n",
            "100%|██████████| 3/3 [09:56<00:00, 198.79s/it]\n",
            "\n",
            " 33%|███▎      | 1/3 [09:56<19:52, 596.38s/it]\u001b[A\n",
            "\n",
            "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            " 33%|███▎      | 1/3 [05:19<10:38, 319.47s/it]\u001b[A\u001b[A\n",
            "\n",
            " 67%|██████▋   | 2/3 [11:52<06:02, 362.55s/it]\u001b[A\u001b[A\n",
            "\n",
            "100%|██████████| 3/3 [19:50<00:00, 396.68s/it]\n",
            "\n",
            " 67%|██████▋   | 2/3 [29:46<15:45, 945.59s/it]\u001b[A\n",
            "\n",
            "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            " 33%|███▎      | 1/3 [07:51<15:43, 471.81s/it]\u001b[A\u001b[A\n",
            "\n",
            " 67%|██████▋   | 2/3 [17:41<09:01, 541.04s/it]\u001b[A\u001b[A\n",
            "\n",
            "100%|██████████| 3/3 [29:37<00:00, 592.66s/it]\n",
            "\n",
            "100%|██████████| 3/3 [59:24<00:00, 1188.14s/it]\n",
            "100%|██████████| 4/4 [3:55:37<00:00, 3534.29s/it]\n"
          ]
        }
      ],
      "source": [
        "model_list = []\n",
        "lr_list = [0.1, 0.05, 0.01, 0.001]\n",
        "estimator_list = [100, 200, 300]\n",
        "depth_list = [5, 6, 7]\n",
        "train_x, train_y, train_group = make_group(list(new_train_data['qid']), train_x_data, train_y_data)\n",
        "\n",
        "for lr in tqdm(lr_list):\n",
        "  for est in tqdm(estimator_list):\n",
        "    for dep in tqdm(depth_list):\n",
        "      model = xgb.XGBRanker(\n",
        "          booster = 'gbtree',\n",
        "          objective = 'rank:pairwise',\n",
        "          eta = lr,\n",
        "          max_depth = dep,\n",
        "          n_estimators = est\n",
        "      )\n",
        "      model_list.append(model.fit(train_x, train_y, group = train_group, verbose = 0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "a_988otuuNFz"
      },
      "outputs": [],
      "source": [
        "def gengerate_LM_score(qid, model, queries_embedding, passages_embedding):\n",
        "  \n",
        "  qid_dict = {}\n",
        "  qid_vector = queries_embedding[qid]\n",
        "  pid_index = [i for i,x in enumerate(validation_qid_list) if x == qid ]\n",
        "  pred_list = []\n",
        "  for index in pid_index:\n",
        "    pid = validation_pid_list[index]\n",
        "    pid_vector = passages_embedding[pid]\n",
        "    qid_pid_vector = np.hstack((qid_vector, pid_vector))\n",
        "    qid_pid_vector = qid_pid_vector.reshape(-1)\n",
        "    pred_list.append(qid_pid_vector)\n",
        "  \n",
        "  pred_array = np.array(pred_list)\n",
        "  pred = model.predict(pred_array)\n",
        "\n",
        "  for i in range(len(pred)):\n",
        "    pid = validation_pid_list[pid_index[i]]\n",
        "    score = float(pred[i])\n",
        "    add_dict = {pid:score}\n",
        "    qid_dict.update(add_dict)\n",
        "\n",
        "  sorted_top_100 = dict(sorted(qid_dict.items(), key=itemgetter(1), reverse = True)[: 100])\n",
        "\n",
        "  return sorted_top_100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n2o4qUPaNTjH",
        "outputId": "b0e54e86-bc37-46d4-8877-50055a3d3f36"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 36/36 [1:03:50<00:00, 106.41s/it]\n"
          ]
        }
      ],
      "source": [
        "score_dict_list = []\n",
        "for i in tqdm(range(len(model_list))):\n",
        "  LM_dict = {}\n",
        "  for qid in validation_qid_list:\n",
        "    if qid in LM_dict.keys():\n",
        "      continue\n",
        "    else:\n",
        "      qid_score_dict = gengerate_LM_score(qid, model_list[i], validation_queries_embedding, validation_passages_embedding)\n",
        "      add_dict = {qid: qid_score_dict}\n",
        "      LM_dict.update(add_dict)\n",
        "  score_dict_list.append(LM_dict)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hrLusKGx25_Q",
        "outputId": "51dd066d-c8df-462f-8484-65f34965201a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MAP for model 1: 0.012821884082367141\n",
            "MAP for model 2: 0.010564471085824244\n",
            "MAP for model 3: 0.010066165931645723\n",
            "MAP for model 4: 0.01036002140485415\n",
            "MAP for model 5: 0.009682296644411647\n",
            "MAP for model 6: 0.009905109325958155\n",
            "MAP for model 7: 0.010366439657542975\n",
            "MAP for model 8: 0.00909928625117986\n",
            "MAP for model 9: 0.009570690510520853\n",
            "MAP for model 10: 0.010721145410539803\n",
            "MAP for model 11: 0.01204384717798579\n",
            "MAP for model 12: 0.013116383530122873\n",
            "MAP for model 13: 0.011025947851044503\n",
            "MAP for model 14: 0.012012977642358393\n",
            "MAP for model 15: 0.010256827987842916\n",
            "MAP for model 16: 0.010428246595554353\n",
            "MAP for model 17: 0.010244443742248343\n",
            "MAP for model 18: 0.009512697653894975\n",
            "MAP for model 19: 0.009626103741977659\n",
            "MAP for model 20: 0.010067879641273165\n",
            "MAP for model 21: 0.008712986708855253\n",
            "MAP for model 22: 0.010856201511759415\n",
            "MAP for model 23: 0.00921308699965868\n",
            "MAP for model 24: 0.00932692742282686\n",
            "MAP for model 25: 0.009190556583230794\n",
            "MAP for model 26: 0.009052930491427313\n",
            "MAP for model 27: 0.009230828427382593\n",
            "MAP for model 28: 0.008960113049123267\n",
            "MAP for model 29: 0.009209798974310702\n",
            "MAP for model 30: 0.008572954696569457\n",
            "MAP for model 31: 0.008964738165655597\n",
            "MAP for model 32: 0.00932577704886043\n",
            "MAP for model 33: 0.009193838454286037\n",
            "MAP for model 34: 0.00911945052392466\n",
            "MAP for model 35: 0.010048525210781045\n",
            "MAP for model 36: 0.009181451908197795\n"
          ]
        }
      ],
      "source": [
        "for i in range(len(score_dict_list)):\n",
        "  LM_AP_list = generate_AP(score_dict_list[i], validation_rel_dic, validation_non_rel_dic)\n",
        "  LM_AP = np.mean(LM_AP_list)\n",
        "  print('MAP for model ' + str(i + 1) + ': ' + str(LM_AP))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vaHDHIS73vmZ",
        "outputId": "a2066085-b510-4342-9e17-6adbf4ca9d35"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NDCG for model 1: 0.03293901744426016\n",
            "NDCG for model 2: 0.03153329243651167\n",
            "NDCG for model 3: 0.02908159330345498\n",
            "NDCG for model 4: 0.029713059623724263\n",
            "NDCG for model 5: 0.03080196384328192\n",
            "NDCG for model 6: 0.03115271762781139\n",
            "NDCG for model 7: 0.03054438482160823\n",
            "NDCG for model 8: 0.02907099510940176\n",
            "NDCG for model 9: 0.029946105774373382\n",
            "NDCG for model 10: 0.030969871974781783\n",
            "NDCG for model 11: 0.032562280345111105\n",
            "NDCG for model 12: 0.03379108869328335\n",
            "NDCG for model 13: 0.030700833505978915\n",
            "NDCG for model 14: 0.033151321990398296\n",
            "NDCG for model 15: 0.03008007415084328\n",
            "NDCG for model 16: 0.03090258341858005\n",
            "NDCG for model 17: 0.029484425473997093\n",
            "NDCG for model 18: 0.02932160247456011\n",
            "NDCG for model 19: 0.031160939630486946\n",
            "NDCG for model 20: 0.03113836993647027\n",
            "NDCG for model 21: 0.03090392730827075\n",
            "NDCG for model 22: 0.03105586275781211\n",
            "NDCG for model 23: 0.03177943151703813\n",
            "NDCG for model 24: 0.030156630657410224\n",
            "NDCG for model 25: 0.02902874866833616\n",
            "NDCG for model 26: 0.02955258151962719\n",
            "NDCG for model 27: 0.03123470323177013\n",
            "NDCG for model 28: 0.026396123696790248\n",
            "NDCG for model 29: 0.027406025500795066\n",
            "NDCG for model 30: 0.02729701693635288\n",
            "NDCG for model 31: 0.02689204537689437\n",
            "NDCG for model 32: 0.028624371181937724\n",
            "NDCG for model 33: 0.029064897789402733\n",
            "NDCG for model 34: 0.02768837676026745\n",
            "NDCG for model 35: 0.029439434821064567\n",
            "NDCG for model 36: 0.029744304149249237\n"
          ]
        }
      ],
      "source": [
        "for i in range(len(score_dict_list)):\n",
        "  LM_NDCG_list = generate_NDCG(score_dict_list[i], validation_rel_dic, validation_non_rel_dic)\n",
        "  LM_NDCG = np.mean(LM_NDCG_list)\n",
        "  print('NDCG for model ' + str(i + 1) + ': ' + str(LM_NDCG))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xbsjJ6iKjO0c"
      },
      "outputs": [],
      "source": [
        "LM_dict= score_dict_list[11]\n",
        "with open('LM.txt','w') as f:\n",
        "    for i in range(len(LM_dict.keys())):\n",
        "        qid = list(LM_dict.keys())[i]\n",
        "        pids = list(LM_dict[qid].keys())\n",
        "        #if not equals 100, delete it\n",
        "        if len(pids) < 100:\n",
        "          continue\n",
        "        for j in range(100):\n",
        "          pid = pids[j]\n",
        "          # qid A2 pid rank score algoname\n",
        "          f.writelines([str(qid), '  A2  ', str(pid),'  ', str(j+1),'  ',str(float(LM_dict[qid][pid])), '  LM', '\\n'])\n",
        "f.close()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
