{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bzqx-nYa6Rlu",
        "outputId": "c093b09c-dbcc-4255-c6f0-00e4259c8531"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader,Dataset\n",
        "from torch.autograd import Variable\n",
        "import string\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import csv\n",
        "import pandas as pd\n",
        "import re\n",
        "import os\n",
        "import nltk\n",
        "from nltk.corpus import stopwords \n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk.stem import SnowballStemmer \n",
        "import collections\n",
        "from collections import Counter\n",
        "import random\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models.word2vec import LineSentence\n",
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "from tqdm import tqdm\n",
        "from operator import itemgetter\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import Adam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "MHvpDQ4i6pLu"
      },
      "outputs": [],
      "source": [
        "def preprocessing(text, remove):\n",
        "    #initial setting\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    stemmer = SnowballStemmer('english')\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokenizer = RegexpTokenizer(r'\\s+', gaps = True)\n",
        "    processed_txt = []\n",
        "\n",
        "    for line in text:\n",
        "        sentence = []\n",
        "        #remove the punctuation\n",
        "        table = str.maketrans(dict.fromkeys(string.punctuation))\n",
        "        line = line.translate(table)\n",
        "\n",
        "        line = re.sub(r'[^a-zA-Z\\s]', u' ', line, flags = re.UNICODE)\n",
        "\n",
        "        line_token = tokenizer.tokenize(line)\n",
        "\n",
        "        #remove stop words\n",
        "        if (remove == True):\n",
        "            line_token = [w for w in line_token if not w in stop_words]\n",
        "        \n",
        "\n",
        "        for word in line_token:\n",
        "            word = lemmatizer.lemmatize(word)\n",
        "            word = stemmer.stem(word)\n",
        "            sentence.append(word)\n",
        "\n",
        "        processed_txt.append(sentence)\n",
        "    \n",
        "    return processed_txt\n",
        "\n",
        "def generate_rel_dict(qid_list, pid_list, rel_list):\n",
        "    '''\n",
        "    generate two dict according to the given data\n",
        "    '''\n",
        "\n",
        "    rel_dict = {}\n",
        "    non_rel_dict = {}\n",
        "\n",
        "    for i in range(len(qid_list)):\n",
        "        qid = qid_list[i]\n",
        "        pid = pid_list[i]\n",
        "        rel = rel_list[i]\n",
        "\n",
        "        if rel > 0:\n",
        "            add_dict = {pid:i}\n",
        "            if qid in rel_dict.keys():\n",
        "                rel_dict[qid].update(add_dict)\n",
        "            else:\n",
        "                rel_dict[qid] = add_dict\n",
        "        else:\n",
        "            add_dict = {pid:i}\n",
        "            if qid in non_rel_dict.keys():\n",
        "                non_rel_dict[qid].update(add_dict)\n",
        "            else:\n",
        "                non_rel_dict[qid] = add_dict\n",
        "\n",
        "    return rel_dict, non_rel_dict\n",
        "\n",
        "def generate_AP(model, rel_dic, non_rel_dic):\n",
        "    \n",
        "    total_ap = []\n",
        "    qid_list = list(model.keys())\n",
        "\n",
        "    for qid in qid_list:\n",
        "        N = 0\n",
        "        R_rel = 0\n",
        "        precision = 0\n",
        "        model_pid_list= list(model[qid].keys())\n",
        "        rel_pid_list = list(rel_dic[qid].keys())\n",
        "        \n",
        "\n",
        "        if len(set(model_pid_list) & set(rel_pid_list)) == 0:\n",
        "            precision = 0\n",
        "            total_ap.append(precision)\n",
        "\n",
        "        else:\n",
        "            for pid in model_pid_list:\n",
        "                N += 1\n",
        "                if pid in rel_pid_list:\n",
        "                    R_rel += 1\n",
        "                    precision += R_rel/N\n",
        "                else:\n",
        "                    continue\n",
        "                if R_rel == len(rel_dic[qid]):\n",
        "                    break\n",
        "\n",
        "        \n",
        "            total_ap.append(precision / R_rel)\n",
        "    \n",
        "    return total_ap\n",
        "\n",
        "def generate_NDCG(model, rel_dic, non_rel_dic):\n",
        "\n",
        "    NDCG = []\n",
        "    qid_list = list(model.keys())\n",
        "    \n",
        "    for qid in qid_list:\n",
        "        N = 0\n",
        "        N_opt = 0\n",
        "        DCG = 0\n",
        "        DCG_opt = 0\n",
        "        model_pid_list = list(model[qid].keys())\n",
        "        rel_pid_list = list(rel_dic[qid].keys())\n",
        "\n",
        "        if len(set(model_pid_list) & set(rel_pid_list)) == 0:\n",
        "            DCG += 0\n",
        "        \n",
        "\n",
        "        else:\n",
        "            rel_qid_dic = rel_dic[qid]\n",
        "            for pid in model_pid_list:\n",
        "                N += 1\n",
        "                if pid in rel_pid_list:\n",
        "                    rel_pid = 1\n",
        "                else:\n",
        "                    rel_pid = 0\n",
        "            \n",
        "                DCG += (2**rel_pid - 1)/np.log(1 + N)\n",
        "\n",
        "        #find the opt DCG\n",
        "        rel_qid_dic = rel_dic[qid]\n",
        "        best_sort_ranking = dict(sorted(rel_qid_dic.items(), key=itemgetter(1), reverse = True)[: 100])\n",
        "        opt_pid_list = list(best_sort_ranking.keys())\n",
        "        \n",
        "        for pid in opt_pid_list:\n",
        "            rel_pid = 1\n",
        "            N_opt += 1\n",
        "            DCG_opt += (2**rel_pid - 1)/np.log(1 + N_opt)\n",
        "\n",
        "        NDCG.append(DCG / DCG_opt)\n",
        "    \n",
        "\n",
        "    return NDCG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "t7p2Jo1s67_a"
      },
      "outputs": [],
      "source": [
        "train_data = pd.read_csv('train_data.tsv', sep = '\\t', header = 0)\n",
        "validation_data = pd.read_csv('validation_data.tsv', sep = '\\t', header = 0)\n",
        "\n",
        "train_qid_dict = np.load('train_qid_dict.npy', allow_pickle= True).tolist()\n",
        "train_pid_dict = np.load('train_pid_dict.npy', allow_pickle= True).tolist()\n",
        "\n",
        "validation_qid_dict = np.load('validation_qid_dict.npy', allow_pickle= True).tolist()\n",
        "validation_pid_dict = np.load('validation_pid_dict.npy', allow_pickle= True).tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "8oCgFPnw7Fft"
      },
      "outputs": [],
      "source": [
        "train_qid_list = list(train_data['qid'])\n",
        "train_pid_list = list(train_data['pid'])\n",
        "\n",
        "\n",
        "train_rel_list = list(train_data['relevancy'])\n",
        "train_rel_dic, train_non_rel_dic = generate_rel_dict(train_qid_list, train_pid_list, train_rel_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qV-DSpCG7bu6",
        "outputId": "6b2bc00c-9391-4938-ef6a-af0e090d762a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "the number of revelance rows is 4797\n",
            "the number of non-revelance rows is 4359542\n"
          ]
        }
      ],
      "source": [
        "def get_row(dict):\n",
        "    key_list = list(dict.keys())\n",
        "    len_row = 0\n",
        "    for key in key_list:\n",
        "        len_row += len(dict[key])\n",
        "\n",
        "    return len_row\n",
        "\n",
        "print('the number of revelance rows is', get_row(train_rel_dic))#4797\n",
        "print('the number of non-revelance rows is', get_row(train_non_rel_dic))#4359542"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2mm8KfbB7i--",
        "outputId": "b6312d36-d853-4a5e-ee35-ff9db9d5effe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "current length of new dataset is 87647\n"
          ]
        }
      ],
      "source": [
        "def subsampling(pid_dict, non_rel_dic):\n",
        "    \n",
        "    qid_list = list(non_rel_dic.keys())\n",
        "\n",
        "\n",
        "    save_index_list = []\n",
        "    for qid in qid_list:\n",
        "\n",
        "        non_rel_pid_dict = non_rel_dic[qid]\n",
        "\n",
        "        non_rel_index_list = list(non_rel_pid_dict.values())\n",
        "        save_len = int(len(non_rel_index_list) * 0.02)\n",
        "        #shuffle the pid then delete by specific ratio\n",
        "        random.shuffle(non_rel_index_list)\n",
        "        new_non_rel_index_list = non_rel_index_list[0:save_len]\n",
        "\n",
        "        #upadte the pid revelant\n",
        "        save_index_list.extend(new_non_rel_index_list)\n",
        "\n",
        "        #new_rel_dic[qid] = add_non_rel\n",
        "\n",
        "    #new_pid_dict = {key:val for key, val in pid_dict.items() if key in save_pid_list}\n",
        "\n",
        "    return save_index_list\n",
        "\n",
        "new_non_index = subsampling(train_pid_dict, train_non_rel_dic)\n",
        "\n",
        "\n",
        "\n",
        "rel_index = []\n",
        "for qid in list(train_rel_dic.keys()):\n",
        "    rel_index_list = list(train_rel_dic[qid].values())\n",
        "    rel_index.extend(rel_index_list)\n",
        "\n",
        "new_train_index = new_non_index + rel_index\n",
        "print('current length of new dataset is', len(new_train_index))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YjQx8TPn7leX",
        "outputId": "409323a7-568b-4d01-d92c-12eaa3674f96"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "          qid      pid                                            queries  \\\n",
            "0      188714  8523351         foods and supplements to lower blood sugar   \n",
            "1      188714  6947934         foods and supplements to lower blood sugar   \n",
            "2      188714  7352565         foods and supplements to lower blood sugar   \n",
            "3      188714  3387416         foods and supplements to lower blood sugar   \n",
            "4      188714  1130808         foods and supplements to lower blood sugar   \n",
            "...       ...      ...                                                ...   \n",
            "87642  401287   860900  is a written prescription required for hydroco...   \n",
            "87643  541272   876066                        was wilson a good president   \n",
            "87644  845529   882642              what is the salary range of a dentist   \n",
            "87645  850361   926854              what is the temperature in washington   \n",
            "87646  969974   956426               where did the the trail of tears end   \n",
            "\n",
            "                                                 passage  relevancy  \n",
            "0      Yes. Certain foods are packed with nutrients t...        0.0  \n",
            "1      One of the fastest ways around for lowering yo...        0.0  \n",
            "2      Folate is a form of B vitamin that occurs natu...        0.0  \n",
            "3      Some foods quickly spike your blood sugar, whi...        0.0  \n",
            "4      Starting your day with a blood sugar stabilizi...        0.0  \n",
            "...                                                  ...        ...  \n",
            "87642  Refills of hydrocodone combination products wi...        1.0  \n",
            "87643  Woodrow Wilson (1856-1924), the 28th U.S. pres...        1.0  \n",
            "87644  Dentist Salary. (United States). The average s...        1.0  \n",
            "87645  July is the hottest month in Washington DC wit...        1.0  \n",
            "87646  The Trail of Tears. The Indian-removal process...        1.0  \n",
            "\n",
            "[87647 rows x 5 columns]\n"
          ]
        }
      ],
      "source": [
        "new_train_data = []\n",
        "for index in new_train_index:\n",
        "    new_train_data.append(train_data[index:index + 1])\n",
        "\n",
        "new_train_data = pd.concat(new_train_data, axis = 0, ignore_index=True)\n",
        "\n",
        "print(new_train_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ah2Aec1AEGO",
        "outputId": "0f1a6b02-0387-40bb-b312-baa6721e3644"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "400002"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "embedding_dict = {}\n",
        "with open(\"glove.6B.100d.txt\", 'r', encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        vector = np.asarray(values[1:], 'float32')\n",
        "        embedding_dict[word] = vector\n",
        "\n",
        "len(embedding_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "0zkLpkAtA1X3"
      },
      "outputs": [],
      "source": [
        "max_length = 200"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "xVycig83Ciq8"
      },
      "outputs": [],
      "source": [
        "train_passages = list(train_pid_dict.values())\n",
        "train_queries = list(train_qid_dict.values())\n",
        "test_passages = list(validation_pid_dict.values())\n",
        "test_queries = list(validation_qid_dict.values())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "pf5bV3iGDM0I"
      },
      "outputs": [],
      "source": [
        "def word_table(datasets,model):\n",
        "    token_index_dict = {} # tokens to indexes\n",
        "    index_vector_dict = {} # indexes to word vectors\n",
        "    i = 0\n",
        "    \n",
        "    for dataset in tqdm(datasets):\n",
        "        for sentence in dataset: # for each query/passage\n",
        "            for token in sentence: # for each token of the sentence\n",
        "                # if this word is not token_to_ind\n",
        "                if(token_index_dict.get(token) == None):\n",
        "                    if token in model.keys():\n",
        "                    # if this word exists is the word model\n",
        "                        i += 1\n",
        "                        token_index_dict[token] = i\n",
        "                        index_vector_dict[i] = model[token]\n",
        "\n",
        "    return token_index_dict, index_vector_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MdAi50YUEGrM",
        "outputId": "e825dfb9-2e24-415b-cd0b-543be0bc3844"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4/4 [00:23<00:00,  5.90s/it]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "124824"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "token_index, index_vector = word_table([train_passages, train_queries, test_passages, test_queries], embedding_dict)\n",
        "len(token_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "RmjeSN78FwVY"
      },
      "outputs": [],
      "source": [
        "def generate_idx(text, token_index_dict, max_length):\n",
        "\n",
        "  return_list = []\n",
        "  for sentence in text:\n",
        "    sen_list = []\n",
        "    for token in sentence:\n",
        "      if (token_index_dict.get(token) != None):\n",
        "        sen_list.append(token_index_dict[token])\n",
        "    \n",
        "    #padding\n",
        "    if len(sen_list) < max_length:\n",
        "      sen_list.extend(0 for _ in range(abs(len(sen_list) - max_length)))\n",
        "\n",
        "  return_list.append(sen_list)\n",
        "\n",
        "  return np.array(return_list)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NXEORDnddBO0",
        "outputId": "19bfe85f-5fb8-4af5-ef7b-d3656221d199"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1, 200)"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "idx = generate_idx(train_passages[100], token_index, max_length)\n",
        "idx.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "manDeWs3To69"
      },
      "outputs": [],
      "source": [
        "\n",
        "class datas(Dataset):\n",
        "  def __init__(self, df, qid_dict, pid_dict):\n",
        "\n",
        "    self.qid_list = df['qid'].values\n",
        "    self.pid_list = df['pid'].values\n",
        "\n",
        "    self.label_list = df['relevancy'].values\n",
        "\n",
        "    self.qid_dict = qid_dict\n",
        "    self.pid_dict = pid_dict\n",
        "\n",
        "    self.length = len(self.qid_list)\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.length\n",
        "  \n",
        "  def __getitem__(self, index):\n",
        "    qid = self.qid_list[index]\n",
        "    pid = self.pid_list[index]\n",
        "\n",
        "    label = self.label_list[index]\n",
        "\n",
        "    idx_qid = generate_idx([self.qid_dict[qid]], token_index, 200)\n",
        "    idx_pid = generate_idx([self.pid_dict[pid]], token_index, 200)\n",
        "\n",
        "    idx_input = np.concatenate((idx_pid, idx_qid), axis = 1)\n",
        "    \n",
        "    return idx_input, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "h7zRUw_bkoLR"
      },
      "outputs": [],
      "source": [
        "class TextCNN(nn.Module):\n",
        "  def __init__(self,vocab_size = len(token_index), embedding_dim = 200, dropout = 0.5):\n",
        "      super(TextCNN, self).__init__()\n",
        "\n",
        "      self.embed = nn.Embedding(vocab_size, embedding_dim)\n",
        "      \n",
        "      self.convs = nn.ModuleList(\n",
        "          [nn.Conv2d(1, 256, (k, 200)) for k in (2, 3, 4)]\n",
        "      )\n",
        "      self.dropout = nn.Dropout(dropout)\n",
        "      self.fc = nn.Linear(256*3, 1)\n",
        "\n",
        "\n",
        "  def conv_and_pool_layers(self, x , conv):\n",
        "    x = conv(x)\n",
        "    x = F.relu(x)\n",
        "    x = x.squeeze(3)\n",
        "    x = F.max_pool1d(x, x.size(2))\n",
        "    x = x.squeeze(2)\n",
        "\n",
        "    return x\n",
        "\n",
        "  def forward(self, input_idx):\n",
        "\n",
        "    \n",
        "    out = self.embed(input_idx)\n",
        "    #print(out.shape)\n",
        "    #out = out.unsqueeze(1)\n",
        "\n",
        "    out = torch.cat([self.conv_and_pool_layers(out, conv) for conv in self.convs], 1)\n",
        "\n",
        "    out = self.dropout(out)\n",
        "\n",
        "    #print(out.shape)\n",
        "\n",
        "    result = self.fc(out)\n",
        "    result = nn.Sigmoid()(result)\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "TC6V8qTEnk68"
      },
      "outputs": [],
      "source": [
        "def train(model, train, lr = 1e-6, epochs = 5):\n",
        "\n",
        "  train_data = datas(train, train_qid_dict, train_pid_dict)\n",
        "  train_data_dataloader = DataLoader(train_data, batch_size = 15, shuffle = True)\n",
        "\n",
        "  criterion = nn.BCELoss()\n",
        "  optimizer = Adam(model.parameters(), lr = lr)\n",
        "\n",
        "  use_cuda = torch.cuda.is_available()\n",
        "  device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "  if use_cuda:\n",
        "    model = model.cuda()\n",
        "    criterion = criterion.cuda()\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "\n",
        "    train_acc = 0\n",
        "    train_loss = 0\n",
        "    \n",
        "    for idx_input, train_label in tqdm(train_data_dataloader):\n",
        "\n",
        "      train_label = train_label.to(device)\n",
        "      idx_input = idx_input.to(device)\n",
        "      #model.to(device)\n",
        "\n",
        "      \n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      output = model(idx_input)\n",
        "      batch_loss = criterion(output.squeeze(), train_label.to(torch.float32))\n",
        "\n",
        "      #optimizer.zero_grad()\n",
        "      train_loss += batch_loss.item()\n",
        "\n",
        "\n",
        "      #acc = int(torch.count_nonzero(output.squeeze() != train_label.to(torch.float32)))\n",
        "      \n",
        "      #train_acc += acc\n",
        "\n",
        "      batch_loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "\n",
        "    print(f'''Epochs: {epoch + 1} | train loss: {train_loss / len(train): .3f}\n",
        "          ''')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6HVLt8T_o48k",
        "outputId": "ad3be791-1f23-40b6-9c15-82ab870ba73f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 5844/5844 [02:41<00:00, 36.23it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epochs: 1 | train loss:  0.020\n",
            "          \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 5844/5844 [02:36<00:00, 37.34it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epochs: 2 | train loss:  0.015\n",
            "          \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 5844/5844 [02:36<00:00, 37.35it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epochs: 3 | train loss:  0.015\n",
            "          \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 5844/5844 [02:36<00:00, 37.31it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epochs: 4 | train loss:  0.015\n",
            "          \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 5844/5844 [02:37<00:00, 37.13it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epochs: 5 | train loss:  0.015\n",
            "          \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "model = TextCNN()\n",
        "train(model, new_train_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "CSjwiecl7Di0"
      },
      "outputs": [],
      "source": [
        "#torch.save(model, '/content/drive/MyDrive/model_cnn.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rHzNkQEsXgGa"
      },
      "outputs": [],
      "source": [
        "#model = torch.load('model_cnn.pth')\n",
        "#model.cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "yjWTqdHeLLtR"
      },
      "outputs": [],
      "source": [
        "val_set = datas(validation_data, validation_qid_dict, validation_pid_dict)\n",
        "val_loader = DataLoader(val_set, batch_size = 3, shuffle = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7cgulZPBLO_I"
      },
      "outputs": [],
      "source": [
        "y_pred = []\n",
        "with torch.no_grad():\n",
        "      model.eval()\n",
        "      for idx_input, train_label in tqdm(val_loader):\n",
        "        \n",
        "        \n",
        "        use_cuda = torch.cuda.is_available()\n",
        "        device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "        #device = torch.device(\"cpu\")\n",
        "\n",
        "        model.to(device)\n",
        "        train_label = train_label.to(device)\n",
        "        idx_input = idx_input.to(device)\n",
        "\n",
        "        output = model(idx_input)\n",
        "        \n",
        "        output = output.squeeze()\n",
        "        output = output.cpu()\n",
        "        output = output.numpy()\n",
        "        output = output.tolist()\n",
        "        if (type(output) == float):\n",
        "          y_pred.append(output)\n",
        "        else:\n",
        "          y_pred.extend(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3_mCl6THjyec"
      },
      "outputs": [],
      "source": [
        "validation_qid_list = list(validation_data['qid'])\n",
        "validation_pid_list = list(validation_data['pid'])\n",
        "validation_rel_list = list(validation_data['relevancy'])\n",
        "CNN_dict = {}\n",
        "for qid in tqdm(list(validation_qid_dict.keys())):\n",
        "  pid_index = [i for i,x in enumerate(validation_qid_list) if x == qid ]\n",
        "  qid_dict = {}\n",
        "  for i in pid_index:\n",
        "    y_pid = y_pred[i]\n",
        "    pid = validation_pid_list[i]\n",
        "    add_dict = {pid:y_pid}\n",
        "    qid_dict.update(add_dict)\n",
        "\n",
        "  sorted_top_100 = dict(sorted(qid_dict.items(), key=itemgetter(1), reverse = True)[: 100])\n",
        "  CNN_dict.update({qid: sorted_top_100})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z_llZycTlmTv"
      },
      "outputs": [],
      "source": [
        "def generate_rel_dict(qid_list, pid_list, rel_list):\n",
        "    '''\n",
        "    generate two dict according to the given data\n",
        "    '''\n",
        "\n",
        "    rel_dict = {}\n",
        "    non_rel_dict = {}\n",
        "\n",
        "    for i in range(len(qid_list)):\n",
        "        qid = qid_list[i]\n",
        "        pid = pid_list[i]\n",
        "        rel = rel_list[i]\n",
        "\n",
        "        if rel > 0:\n",
        "            add_dict = {pid:i}\n",
        "            if qid in rel_dict.keys():\n",
        "                rel_dict[qid].update(add_dict)\n",
        "            else:\n",
        "                rel_dict[qid] = add_dict\n",
        "        else:\n",
        "            add_dict = {pid:i}\n",
        "            if qid in non_rel_dict.keys():\n",
        "                non_rel_dict[qid].update(add_dict)\n",
        "            else:\n",
        "                non_rel_dict[qid] = add_dict\n",
        "\n",
        "    return rel_dict, non_rel_dict\n",
        "\n",
        "\n",
        "validation_rel_dic, validation_non_rel_dic = generate_rel_dict(validation_qid_list, validation_pid_list, validation_rel_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B9LMddBzlolT"
      },
      "outputs": [],
      "source": [
        "NN_AP_list = generate_AP(CNN_dict, validation_rel_dic, validation_non_rel_dic)\n",
        "NN_AP = np.mean(NN_AP_list)\n",
        "NN_AP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T9A7FrsIlo8O"
      },
      "outputs": [],
      "source": [
        "NN_NDCG_list = generate_NDCG(CNN_dict, validation_rel_dic, validation_non_rel_dic)\n",
        "NN_NDCG = np.mean(NN_NDCG_list)\n",
        "NN_NDCG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "egWUpWcJj1DB"
      },
      "outputs": [],
      "source": [
        "with open('NN.txt','w') as f:\n",
        "    for i in range(len(CNN_dict.keys())):\n",
        "        qid = list(CNN_dict.keys())[i]\n",
        "        pids = list(CNN_dict[qid].keys())\n",
        "        #if not equals 100, delete it\n",
        "        if len(pids) < 100:\n",
        "          continue\n",
        "        for j in range(100):\n",
        "          pid = pids[j]\n",
        "          # qid A2 pid rank score algoname\n",
        "          f.writelines([str(qid), '  A2  ', str(pid),'  ', str(j+1),'  ',str(float(CNN_dict[qid][pid])), '  NN', '\\n'])\n",
        "f.close()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
